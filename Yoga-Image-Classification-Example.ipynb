{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714df8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from keras.applications.vgg19 import VGG19\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e632c09",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Our data will is in the DATASET folder split into our TRAIN and TEST datasets. Each folder contains one folder for each of our labels. Our next step will be to preprocess our data. \n",
    "\n",
    "* We use ImageDataGenerator to do some transformations of the images\n",
    "* Then we can apply the transformations to the directories for our train and test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20762ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/TRAIN'\n",
    "test_dir = 'data/TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0281467",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(width_shift_range = 0.1,\n",
    "                                  horizontal_flip = True,\n",
    "                                  rescale = 1./255,\n",
    "                                  validation_split = 0.2,\n",
    "                                  preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(rescale =1./255,\n",
    "                                 validation_split = 0.2,\n",
    "                                 preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a371cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 866 images belonging to 5 classes.\n",
      "Found 92 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                   target_size = (224,224),\n",
    "                                                   color_mode = 'rgb',\n",
    "                                                   class_mode = 'categorical',\n",
    "                                                    batch_size = 16,\n",
    "                                                   subset = 'training')\n",
    "validation_generator = test_datagen.flow_from_directory(directory = test_dir,\n",
    "                                                       target_size = (224,224),\n",
    "                                                       color_mode = 'rgb',\n",
    "                                                       class_mode = 'categorical',\n",
    "                                                       batch_size = 16,\n",
    "                                                       subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c058182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'downdog': 0, 'goddess': 1, 'plank': 2, 'tree': 3, 'warrior2': 4}\n",
      "{'downdog': 0, 'goddess': 1, 'plank': 2, 'tree': 3, 'warrior2': 4}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.class_indices)\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "122c213c",
   "metadata": {},
   "source": [
    "We will use the VGG19 model you can read more about the requirements and considerations for this model in the documentation (https://keras.io/api/applications/vgg/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ae9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vrive\\Documents\\GitHub\\Yoga-Pose-Detection\\venv\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20024384 (76.39 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 20024384 (76.39 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG19(include_top = False,weights = 'imagenet',input_shape= (224,224,3))\n",
    "\n",
    "# Freeze the imported layers so they cannot be retrained.\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b247471",
   "metadata": {},
   "source": [
    "### Adding flattening and dense layers\n",
    "\n",
    "Right now, our model is missing a top to actually classify our features. Let's add them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "433463e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 125445    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20149829 (76.87 MB)\n",
      "Trainable params: 125445 (490.02 KB)\n",
      "Non-trainable params: 20024384 (76.39 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(5,activation = 'softmax'))\n",
    "\n",
    "# Summarize.\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17de85fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vrive\\Documents\\GitHub\\Yoga-Pose-Detection\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\vrive\\Documents\\GitHub\\Yoga-Pose-Detection\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vrive\\Documents\\GitHub\\Yoga-Pose-Detection\\venv\\Lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 206s 4s/step - loss: 1.3328 - accuracy: 0.4630 - val_loss: 0.7474 - val_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "# Compile and fit the model. Use the Adam optimizer and crossentropical loss. \n",
    "# Use the validation data argument during fitting to include your validation data.\n",
    "optimizer = Adam(learning_rate = 0.0001)\n",
    "new_model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "history = new_model.fit(train_generator,\n",
    "                        epochs=1, \n",
    "                        batch_size=16,\n",
    "                        validation_data=validation_generator\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ca76f",
   "metadata": {},
   "source": [
    "# Predicting the class of your image\n",
    "\n",
    "Let's take this bad boy for a spin! Can your image get properly identified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f32a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(1, 224, 224, 3)\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "(1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'downdog'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "# Predict the class of your picture.\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(\"./test_folder/downward_dog_new.jpg\", target_size = (224, 224))\n",
    "\n",
    "\n",
    "img_nparray = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "print(img_nparray.shape)\n",
    "#convert image to array\n",
    "\n",
    "x = preprocess_input(img_nparray).reshape((1,224,224,3))\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "prediction = new_model.predict(x)\n",
    "\n",
    "print(prediction.shape)\n",
    "\n",
    "# create a list containing the class labels\n",
    "class_labels = ['downdog', 'goddess', 'plank', 'tree', 'warrior2']\n",
    "\n",
    "# find the index of the class with maximum score\n",
    "pred = np.argmax(prediction, axis=-1)\n",
    "class_labels[pred[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
